{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mqn6fiX3lMmX"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import itertools\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aOIoKi7mjCG"
      },
      "source": [
        "# Load data and split in the multi agent system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "XhLZzr4rhUAP",
        "outputId": "bd1479ec-62ba-4cf2-a9e9-a59ba7ad2984"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'transforms' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 86\u001b[0m\n\u001b[0;32m     82\u001b[0m test_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m#alpha = 0.80 # Dirichlet distribution parameter\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Load and preprocess CIFAR-10 dataset\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m train_dataset, test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_mnist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Split train dataset into train and validation sets\u001b[39;00m\n\u001b[0;32m     89\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataset))\n",
            "Cell \u001b[1;32mIn[1], line 2\u001b[0m, in \u001b[0;36mload_mnist\u001b[1;34m(train_subset_size, test_subset_size)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_mnist\u001b[39m(train_subset_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15000\u001b[39m, test_subset_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1500\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     transform \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      3\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m      4\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m,), (\u001b[38;5;241m0.5\u001b[39m,))\n\u001b[0;32m      5\u001b[0m     ])\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Load the full datasets\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mMNIST(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'transforms' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "def load_mnist(train_subset_size=15000, test_subset_size=1500):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    # Load the full datasets\n",
        "    train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    # Get the subset of indices\n",
        "    train_indices = np.random.choice(len(train_dataset), train_subset_size, replace=False)\n",
        "    test_indices = np.random.choice(len(test_dataset), test_subset_size, replace=False)\n",
        "\n",
        "    # Create subsets\n",
        "    train_subset = Subset(train_dataset, train_indices)\n",
        "    test_subset = Subset(test_dataset, test_indices)\n",
        "\n",
        "    return train_subset, test_subset\n",
        "\n",
        "# def load_mnist():\n",
        "#     transform = transforms.Compose(\n",
        "#         [transforms.ToTensor(),\n",
        "#          transforms.Normalize((0.5), (0.5))])\n",
        "\n",
        "#     train_dataset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "#                                                  download=True, transform=transform)\n",
        "#     test_dataset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "#                                                 download=True, transform=transform)\n",
        "#     return train_dataset, test_dataset\n",
        "\n",
        "def split_dataset_heterogeneously(dataset, num_agents, alpha=0.8):\n",
        "    np.random.seed(42)\n",
        "    num_samples = len(dataset)\n",
        "    indices = np.random.permutation(num_samples)\n",
        "    # Generate Dirichlet distribution for each agent\n",
        "    proportions = np.random.dirichlet(np.repeat(alpha, num_agents), size=1)[0]\n",
        "    # Calculate the number of samples for each agent\n",
        "    agent_sizes = (proportions * num_samples).astype(int)\n",
        "    # Adjust to ensure all samples are allocated\n",
        "    agent_sizes[-1] = num_samples - np.sum(agent_sizes[:-1])\n",
        "\n",
        "    agent_splits = []\n",
        "    start_idx = 0\n",
        "    for size in agent_sizes:\n",
        "        end_idx = start_idx + size\n",
        "        agent_indices = indices[start_idx:end_idx]\n",
        "        agent_subset = Subset(dataset, agent_indices.tolist())\n",
        "        agent_splits.append(agent_subset)\n",
        "        start_idx = end_idx\n",
        "\n",
        "    return agent_splits, proportions\n",
        "\n",
        "def create_dataloaders(train_splits, val_splits, test_set, batch_size=32, test_batch_size=32):\n",
        "    train_loaders = []\n",
        "    val_loaders = []\n",
        "    test_loader = DataLoader(test_set, batch_size=test_batch_size, shuffle=False)\n",
        "\n",
        "    for train_data, val_data in zip(train_splits, val_splits):\n",
        "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "        train_loaders.append(train_loader)\n",
        "        val_loaders.append(val_loader)\n",
        "\n",
        "    return train_loaders, val_loaders, test_loader\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize if necessary\n",
        "    npimg = img.numpy()\n",
        "\n",
        "    # Check if the image has 1 channel (grayscale)\n",
        "    if npimg.shape[0] == 1:\n",
        "        plt.imshow(npimg[0], cmap='gray')\n",
        "    else:\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Parameters\n",
        "num_agents = 10\n",
        "batch_size = 32\n",
        "test_batch_size = 64\n",
        "#alpha = 0.80 # Dirichlet distribution parameter\n",
        "\n",
        "# Load and preprocess CIFAR-10 dataset\n",
        "train_dataset, test_dataset = load_mnist()\n",
        "\n",
        "# Split train dataset into train and validation sets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Split train and validation sets heterogeneously among agents\n",
        "train_splits, train_proportions = split_dataset_heterogeneously(train_dataset, num_agents)\n",
        "val_splits, val_proportions = split_dataset_heterogeneously(val_dataset, num_agents)\n",
        "\n",
        "# Create dataloaders for each agent\n",
        "train_loaders, val_loaders, test_loader = create_dataloaders(train_splits, val_splits, test_dataset, batch_size, test_batch_size)\n",
        "\n",
        "# Print and plot the first batch of the first agent\n",
        "dataiter = iter(train_loaders[0])\n",
        "images, labels = dataiter.__next__()\n",
        "print(\"First batch of the first agent:\")\n",
        "print(f\"  Images shape: {images.shape}\")\n",
        "print(f\"  Labels: {labels}\")\n",
        "\n",
        "# Plot the images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# Verify the size of the test loader\n",
        "print(f\"Test loader batch size: {test_loader.batch_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I58K5F3-0-Q",
        "outputId": "90ca9938-63d2-4fc8-c29d-f19095cca09d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "695\n",
            "19\n",
            "915\n",
            "261\n",
            "1743\n",
            "204\n",
            "160\n",
            "2531\n",
            "886\n",
            "586\n"
          ]
        }
      ],
      "source": [
        "for i in range(num_agents):\n",
        "  print(len(train_loaders[i].dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPhJ_wwumW3S"
      },
      "source": [
        "# Create Graph Topology"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "d29ny345k-jF",
        "outputId": "365d8ad2-9f5c-4b36-97e8-e4b7b796e78f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def genera_grafo_connesso(tipo_grafo, num_nodi, prob=None, m=None):\n",
        "    while True:\n",
        "        if tipo_grafo == 'erdos':\n",
        "            if prob is None:\n",
        "                raise ValueError(\"Per un grafo di Erdos-Renyi, specificare la probabilità 'prob'\")\n",
        "            G = nx.erdos_renyi_graph(num_nodi, prob)\n",
        "        elif tipo_grafo == 'scale_free':\n",
        "            if m is None:\n",
        "                raise ValueError(\"Per un grafo scale-free, specificare il numero di archi 'm'\")\n",
        "            G = nx.barabasi_albert_graph(num_nodi, m)\n",
        "        else:\n",
        "            raise ValueError(\"Tipo di grafo non supportato. Scegliere tra 'erdos_renyi' o 'scale_free'\")\n",
        "\n",
        "        # Controlla se il grafo è connesso\n",
        "        if nx.is_connected(G):\n",
        "            break\n",
        "\n",
        "    adj_matrix = nx.adjacency_matrix(G).todense()\n",
        "    laplacian_matrix = nx.laplacian_matrix(G).todense()\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    nx.draw(G, with_labels=True, node_color='skyblue', node_size=500, edge_color='gray')\n",
        "    plt.title(f\"Graph {tipo_grafo.replace('_', ' ').title()}\")\n",
        "    plt.show()\n",
        "    return adj_matrix, laplacian_matrix, G\n",
        "\n",
        "type_g = 'erdos'\n",
        "p = 0.25\n",
        "Adj_matrix, L, G = genera_grafo_connesso(type_g, num_agents, prob=p)\n",
        "# print(\"Matrice di adiacenza:\\n\", A)\n",
        "# print(\"Laplaciano:\\n\", L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw-TwGISqiMr"
      },
      "source": [
        "# Define learning function at the edge of the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UC-9VMhqqiWr"
      },
      "outputs": [],
      "source": [
        "# class LeNet(nn.Module):\n",
        "#     def __init__(self, input_channels=1, num_classes=10):\n",
        "#         super(Net, self).__init__()\n",
        "\n",
        "#         # Set the seed for reproducibility\n",
        "#         #torch.manual_seed(0)\n",
        "\n",
        "#         self.conv1 = nn.Conv2d(input_channels, 6, kernel_size=5)\n",
        "#         self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "#         self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "#         self.fc2 = nn.Linear(120, 84)\n",
        "#         self.fc3 = nn.Linear(84, num_classes)\n",
        "\n",
        "#         # Initialize weights\n",
        "#         self._initialize_weights()\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = F.relu(self.conv1(x))\n",
        "#         x = F.max_pool2d(x, 2)\n",
        "#         x = F.relu(self.conv2(x))\n",
        "#         x = F.max_pool2d(x, 2)\n",
        "#         x = x.view(-1, 16 * 4 * 4)  # Flatten the tensor\n",
        "#         x = F.relu(self.fc1(x)) #hyperbolic\n",
        "#         x = F.relu(self.fc2(x))\n",
        "#         x = self.fc3(x)\n",
        "#         return x\n",
        "\n",
        "#     def _initialize_weights(self):\n",
        "#         for m in self.modules():\n",
        "#             if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "#                 init.xavier_normal_(m.weight)\n",
        "#                 if m.bias is not None:\n",
        "#                     init.constant_(m.bias, 0)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, input_channels=1, num_classes=10, randomness=False):\n",
        "        super(Net, self).__init__()\n",
        "        # if randomness==False:\n",
        "        #torch.manual_seed(42)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(input_channels, 8, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(8 * 24 * 24, 32)  # Adjust input size for fully connected layer\n",
        "        self.fc2 = nn.Linear(32, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.gelu(self.conv1(x))  # Using GELU activation function\n",
        "        x = x.view(-1, 8 * 24 * 24)\n",
        "        x = F.gelu(self.fc1(x))\n",
        "        x = self.fc2(x)            # Output layer\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                init.xavier_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pQyqtVUpjOK"
      },
      "source": [
        "# Define Successive Convex Approximation routine:\n",
        "$$ min_x U(x) = F(x)+G(x) \\\\\n",
        "U_i(x) = f_i(\\omega_i,x_i) + \\sum_{j=1}^I f_j(\\omega_i,x_j) + G(x_i) ; i=\\{1,..,I\\}$$\n",
        "\n",
        "- The surrogate function for *full linearization* (FL) of the initial possibly non convex cost function is defined as a proximal gradient procedure: $$\\tilde{f_i}(x_i|x_i[n])= f_i(x_i[n]) + \\nabla f_i(x_i[n])(x_i - x_i[n]) + \\frac{\\tau}{2}||x_i - x_i[n] ||$$\n",
        "That solved in closed form produces:\n",
        "$$\\nabla_{x} \\tilde{F}(x^k|x_i[n])=0 \\\\\n",
        "\\nabla_{x}F(x_i[n]) + \\tau(x^k-x_i[n])=0 \\\\\n",
        "x^{k+1}= x^k - \\frac{1}{\\tau} \\nabla F(x_i[n])$$\n",
        "\n",
        "- Best Response map aims to approximate for each agent $i^{th}$:\n",
        "$$ \\hat{x_i}(x_i[n]) = argmin_{x_i[n] \\in K} \\tilde{f_i}(x_i|x_i[n]) + (x_i-x_i[n]) \\sum_{j \\neq i} ^N \\nabla_x f_j(x_i[n]) $$\n",
        "\n",
        "- Convex smooth combination:\n",
        "$$ x_i^{k+1} = (1-\\gamma^k)x_i + \\gamma^k(\\hat{x_i}(y^k)) $$\n",
        "\n",
        "The main idea is to develop a gradient tracking method to implement a fully distributed framework where $\\sum_{j \\neq i} ^N \\nabla_x f_j(x_i[n])$, that is not distributed since anget $i$ need the knowledge of all $j^{}th$ agents gadients; \\\n",
        "Furthermore, the use of dynamic consensus to update the\n",
        " objective functions of users' subproblems is a novel idea\n",
        " (introduced for the first time in \"NEXT-Di Lorenzo & Scutari\"), which makes the proposed scheme convergent even in the case of nonconvex\n",
        " $F$'s.\n",
        "\n",
        "\n",
        "## BIO-L-NEXT Algorithm\n",
        "COMPLETARE E RIVEDERE QUESTO RIASSUNTO DEL METODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "t4zU5CxoEb2S"
      },
      "outputs": [],
      "source": [
        "def flatten_param(model, input=None):  # Highlight that you need to pass the\n",
        "                                       # client class and their model (agents.models) as input!\n",
        "    flattened_params = []\n",
        "    if input == None:\n",
        "      for n,p in model.items():\n",
        "        flattened_params.append(p.view(-1))\n",
        "    elif input == \"model\":\n",
        "      for param in model.parameters():\n",
        "        flattened_params.append(param.view(-1))  # Flatten the parameter and add to list\n",
        "\n",
        "    flattened_vector = torch.cat(flattened_params)  # Concatenate all flattened parameters into a single vector 1D\n",
        "    return flattened_vector\n",
        "\n",
        "\n",
        "def get_diffusion(Laplacian):\n",
        "    eigv, _ = torch.linalg.eig(torch.tensor(Laplacian,  dtype=torch.float32))\n",
        "    eps = np.random.uniform(0, (2/torch.max(eigv.real).item()))\n",
        "    #eps = (2/torch.max(eigv.real).item())/100\n",
        "    #print(f\"Learning rate is equal to {round(eps,8)}\")\n",
        "    W = torch.eye(num_agents) - eps * Laplacian\n",
        "    return W, eps\n",
        "\n",
        "#Define attraction repulsion function\n",
        "def g(x, z, pie):\n",
        "    y = (flatten_param(x))-(flatten_param(z))\n",
        "    return pie[0]-pie[1]*(-torch.norm(y, p=2)/pie[2])\n",
        "\n",
        "def reshape_flattened_gradients(flattened_gradients, model):\n",
        "    reshaped_gradients = {}\n",
        "    pointer = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        param_shape = param.shape\n",
        "        param_size = param.numel()  # Total number of elements in the parameter tensor\n",
        "        # Extract the portion of flattened gradients corresponding to this parameter\n",
        "        reshaped_grad = flattened_gradients[pointer:pointer + param_size].view(param_shape)\n",
        "       # Store the reshaped gradient in the dictionary\n",
        "        reshaped_gradients[name] = reshaped_grad.clone().detach()\n",
        "        # Move the pointer to the next set of gradients\n",
        "        pointer += param_size\n",
        "    return reshaped_gradients\n",
        "\n",
        "# Agents class for federated learning\n",
        "class Agents:\n",
        "    def __init__(self, train_loaders, val_loaders, test_loader, num_agents, pie=[], tau = None, epochs=2, epsilon=0.8, gamma_zero=0.5, iterations = None):\n",
        "        self.K = iterations\n",
        "        self.pie = pie\n",
        "        self.train_loaders = train_loaders\n",
        "        self.val_loaders = val_loaders\n",
        "        self.test_loader = test_loader\n",
        "        self.num_agents = num_agents\n",
        "        self.epochs = epochs\n",
        "        self.W, self.learning_rate = get_diffusion(L)[0], get_diffusion(L)[1]\n",
        "        self.gamma_values =[gamma_zero] #gamma[0]< 1/eps\n",
        "        self.models = [Net() for _ in range(num_agents)]\n",
        "        self.optimizers = [optim.SGD(model.parameters(), lr= 1/tau ) for model in self.models]\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.validation_accuracies = {i: 0 for i in range(num_agents)}  # Dictionary to store validation accuracies\n",
        "        self.losses = {i: [] for i in range(num_agents)}  # Dictionary to store losses\n",
        "        self.previous_response = [\n",
        "            {k: torch.zeros_like(p) for k, p in self.models[0].state_dict().items()}\n",
        "            for _ in range(self.num_agents)\n",
        "        ] #store parameters initializations and previous weights iteratively\n",
        "\n",
        "        self.actual_grads = [\n",
        "            {k: torch.zeros_like(p) for k, p in self.models[0].state_dict().items()}\n",
        "            for _ in range(self.num_agents)\n",
        "        ]\n",
        "        # Initialize Y[0], Z[0], pie_tilde[0] as lists of dictionaries\n",
        "        self.Y = [\n",
        "            {k: torch.zeros_like(p) for k, p in self.models[0].state_dict().items()}\n",
        "            for _ in range(self.num_agents)\n",
        "        ]\n",
        "        self.pie_tilde = [\n",
        "            {k: torch.zeros_like(p) for k, p in self.models[0].state_dict().items()}\n",
        "            for _ in range(self.num_agents)\n",
        "        ]\n",
        "        self.Z = [\n",
        "            {k: torch.zeros_like(p) for k, p in self.models[0].state_dict().items()}\n",
        "            for _ in range(self.num_agents)\n",
        "        ]\n",
        "        self.next_grads = [\n",
        "            {k: torch.zeros_like(p) for k, p in self.models[0].state_dict().items()}\n",
        "            for _ in range(self.num_agents)\n",
        "        ]\n",
        "        self.stopping_condition = [{agent:[]} for agent in range(self.num_agents)]\n",
        "\n",
        "\n",
        "    def gamma_update(self, epsilon=0.8): #updating step size rule\n",
        "        gamma_next = self.gamma_values[-1]*(1-(epsilon*self.gamma_values[-1])) #gamma[0] < 1/eps and eps in (0,1])\n",
        "        self.gamma_values.append(gamma_next)\n",
        "\n",
        "\n",
        "    def plot_losses(self):\n",
        "        for agent_idx in range(self.num_agents):\n",
        "            plt.plot(self.losses[agent_idx], label=f'Agent {agent_idx}')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.ylim(0,2.5)\n",
        "        #plt.xlim(0,15)\n",
        "        plt.title('NEXT System Loss behaviour')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def initialize_agents(self):\n",
        "        \"\"\"\n",
        "        Initialize agents' models and variables.\n",
        "        \"\"\"\n",
        "        #wk = Net().state_dict()\n",
        "        for i in range(self.num_agents):\n",
        "            wk = Net().state_dict()\n",
        "            self.previous_response[i] = wk #store feasible initializations\n",
        "            w_previous = self.models[i].state_dict()\n",
        "            # Initialize model parameters to wk\n",
        "            for name, param in self.models[i].named_parameters():\n",
        "                if name in wk:\n",
        "                    param.data = wk[name].clone()\n",
        "            self.models[i].train()\n",
        "            optimizer = self.optimizers[i]\n",
        "            #Compute difference among current iterate and previous response variable (x_i - x[n])\n",
        "            difference = [(w_prev-wk) for (n1,w_prev),(n2,wk) in zip(w_previous.items(), self.models[i].state_dict().items())] #computye and flat difference among w_prev and wk\n",
        "            difference_flatten = torch.cat([diff.view(-1) for diff in difference])\n",
        "            all_batch_gradients = []\n",
        "            for inputs, labels in self.train_loaders[i]:\n",
        "                outputs = self.models[i](inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                optimizer.zero_grad()\n",
        "                gradients = torch.autograd.grad(loss, self.models[i].parameters(), retain_graph=True)\n",
        "                pie_flatten = torch.cat([grad.view(-1) for grad in gradients])\n",
        "                all_batch_gradients.append(pie_flatten.clone().detach())  #initialize y_i[k] as ∇f_i[0]\n",
        "                # Surrogate loss\n",
        "                loss_surrogate = loss.clone().detach().requires_grad_(True) + torch.matmul(pie_flatten, difference_flatten)\n",
        "                loss_surrogate.backward()\n",
        "                optimizer.step()\n",
        "            # Update Z\n",
        "            # for name, param in self.Z[i].items():\n",
        "            #     self.Z[i][name] += w_previous[name] + ( self.gamma_values[-1] * (w_previous[name] - wk[name]) )\n",
        "            # Update Y[0]= ∇f_i(w_i[0])\n",
        "            self.Y[i] = torch.mean(torch.stack(all_batch_gradients), dim=0) #gradients mean of batches (not of agents!)\n",
        "            self.pie_tilde[i] = torch.mul(self.Y[i], self.num_agents) - torch.mean(torch.stack(all_batch_gradients), dim=0)\n",
        "            #self.gamma_update()\n",
        "#-------------------------------------------------------------------------------\n",
        "    def run_iterations(self, tolerance = 1e-2):\n",
        "        \"\"\"\n",
        "        Run K iterations of the NEXT algorithm.\n",
        "        Step 1: Local Updates leveraging SCA optimization\n",
        "                Convex smooth with diminishing gamma step size rule\n",
        "        Step 2: Consensus Step to update model paramters\n",
        "        Step 3: Local Variables Updates\n",
        "        \"\"\"\n",
        "\n",
        "        diff = {k: torch.zeros_like(p) for k, p in self.models[0].state_dict().items()}\n",
        "\n",
        "        for k in tqdm(range(self.K),desc= \"Processing NEXT training with neighborhood\"):\n",
        "            # Step 1\n",
        "            for i in range(self.num_agents):\n",
        "\n",
        "                optimizer = self.optimizers[i]\n",
        "                wk = copy.deepcopy(self.models[i].state_dict()) #run this after initialization menas that agent model\n",
        "                                                                #already have feasible initialization of model parameters\n",
        "                all_batch_gradients = []\n",
        "                #Run the surrogate optimization and update parameters with best response map procedure\n",
        "                #BEST RESPONSE MAP\n",
        "                running_loss = 0.0\n",
        "                for inputs, labels in self.train_loaders[i]:\n",
        "                    outputs = self.models[i](inputs)\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "                    optimizer.zero_grad()\n",
        "                    gradients = torch.autograd.grad(loss, self.models[i].parameters(), retain_graph=True)\n",
        "                    pie_flatten = torch.cat([grad.view(-1) for grad in gradients])\n",
        "                    all_batch_gradients.append(pie_flatten.clone().detach())\n",
        "                    # Surrogate loss\n",
        "                    difference_flatten = flatten_param(self.models[i], input=\"model\") - ( torch.cat([p.view(-1) for n,p in self.previous_response[i].items()])  )\n",
        "                    #print(difference_flatten)\n",
        "                    loss_surrogate = loss.clone().requires_grad_(True) + torch.matmul(pie_flatten, difference_flatten).requires_grad_(True)\n",
        "                    # print(torch.matmul(pie_flatten, difference_flatten))\n",
        "                    running_loss += loss_surrogate.item()\n",
        "                    loss_surrogate.backward()\n",
        "                    optimizer.step() #best response map update x_hat\n",
        "\n",
        "                # Update Z as the convex combination (smooth)\n",
        "                diff = 0\n",
        "                for name, p in self.Z[i].items():\n",
        "                    self.Z[i][name] = wk[name] +  torch.mul( (self.models[i].state_dict()[name] - wk[name]), self.gamma_values[-1] )\n",
        "                    diff += torch.sum(self.models[i].state_dict()[name] - wk[name])\n",
        "                print(f\"Difference equal to {diff}\")\n",
        "\n",
        "                # print(f\"∥x_hat(xk) − x[k] ∥ = {diff} for agent {i}\")\n",
        "                #store gradients of the current iteration;\n",
        "                self.actual_grads[i] = torch.mean(torch.stack(all_batch_gradients), dim=0)\n",
        "                #store loss function\n",
        "                avg_loss = running_loss / len(self.train_loaders[i])\n",
        "                self.losses[i].append(avg_loss)\n",
        "\n",
        "            # Step 2: Consensus Step to update model params\n",
        "            for i in range(self.num_agents):\n",
        "                updated_state_dict = {k: torch.zeros_like(p) for k, p in self.models[i].state_dict().items()}\n",
        "                grads_passing = {k: torch.zeros_like(p) for k, p in self.models[i].state_dict().items()}\n",
        "                for j in range(self.num_agents):\n",
        "                  for (name1, w_i), (name2, z_i) in zip(updated_state_dict.items(), self.Z[j].items()):\n",
        "                    g_bio = g(self.Z[i], self.Z[j], self.pie).type(z_i.dtype)\n",
        "                    updated_state_dict[name1] +=torch.mul( z_i, g_bio) * self.W[i][j]  # Accumulate contributions from each j with attraction-repulsion function\n",
        "\n",
        "                #print(updated_state_dict)\n",
        "                # Load updated state dict\n",
        "                self.models[i].load_state_dict(updated_state_dict)\n",
        "                self.previous_response[i] = updated_state_dict #load the consensus based update in the previous\n",
        "                                                               # response variable;\n",
        "\n",
        "                # Compute next gradients\n",
        "                all_batch_gradients = []\n",
        "                for inputs, labels in self.train_loaders[i]:\n",
        "                    outputs = self.models[i](inputs)\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "                    optimizer = self.optimizers[i]\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward(retain_graph=True)\n",
        "                    gradients = torch.autograd.grad(loss, self.models[i].parameters(), retain_graph=True)\n",
        "                    next_grad_flat = torch.cat([grad.view(-1) for grad in gradients])\n",
        "                    all_batch_gradients.append(next_grad_flat)\n",
        "                self.next_grads[i] = torch.mean(torch.stack(all_batch_gradients), dim=0)\n",
        "                #next_grads.append(torch.mean(torch.stack(all_batch_gradients), dim=0))\n",
        "                # Update y_i[k+1]\n",
        "                grads_diff = self.next_grads[i] - self.actual_grads[i]\n",
        "                grads_diff_resh = reshape_flattened_gradients(grads_diff, self.models[i])\n",
        "\n",
        "                for j in range(self.num_agents):\n",
        "                    if j != i:\n",
        "                      y_j = reshape_flattened_gradients( self.Y[j], self.models[0]  ) #reshape in original paramters nn size\n",
        "                      for name,p in y_j.items():\n",
        "                        grads_passing[name] += p*self.W[i][j] #Accumulate contributions from each j^th agent\n",
        "                for  (n1, consensus),(n2, diff) in zip(grads_passing.items(), grads_diff_resh.items()):\n",
        "                    grads_passing[n1] = consensus + diff\n",
        "                self.Y[i] = torch.cat([grad.view(-1) for n,grad in grads_passing.items()])\n",
        "                # Update pie_tilde\n",
        "                self.pie_tilde[i] = torch.mul(self.Y[i], self.num_agents ) - self.next_grads[i]\n",
        "\n",
        "            #check convergence\n",
        "            converged = True\n",
        "            sum_diff=0\n",
        "            i_agent = flatten_param(self.models[0], input=\"model\")\n",
        "            for j_agent in range(0,self.num_agents):\n",
        "              sum_diff += torch.sum(i_agent - flatten_param(self.models[j_agent], input=\"model\"))\n",
        "            avg_diff = torch.abs(sum_diff / (self.num_agents-1) )\n",
        "            print(f\" \\n Average params difference equal to {avg_diff}\")\n",
        "            #if (k + 1) % 20 == 0:\n",
        "            #print(f\"Difference at iteration {k} is equal to {torch.abs(sum_diff)} for agent 0 and {j_agent}\")\n",
        "            if torch.abs(avg_diff) >= tolerance:\n",
        "              converged = False\n",
        "            if converged==True:\n",
        "             print(f\"Convergence reached in {k} iterations.\")\n",
        "             break\n",
        "\n",
        "            # Update gamma\n",
        "            self.gamma_update()\n",
        "            #refresh gradients variables\n",
        "            self.actual_grads = self.next_grads = [\n",
        "                {k: torch.zeros_like(p) for k, p in self.models[0].state_dict().items()}\n",
        "                for _ in range(self.num_agents)\n",
        "            ]\n",
        "            self.next_grads = [\n",
        "                {k: torch.zeros_like(p) for k, p in self.models[0].state_dict().items()}\n",
        "                for _ in range(self.num_agents)\n",
        "            ]\n",
        "\n",
        "\n",
        "    def NEXT_train(self, tolerance=None):\n",
        "        \"\"\"\n",
        "        Execute the full SCA training process.\n",
        "        \"\"\"\n",
        "        self.initialize_agents()\n",
        "        self.run_iterations(tolerance)\n",
        "        self.plot_losses()\n",
        "\n",
        "\n",
        "    def test_agent(self, agent_idx):\n",
        "        model = self.models[agent_idx]\n",
        "        test_loader = self.test_loader\n",
        "        model.eval()\n",
        "\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_accuracy = correct / total\n",
        "        print(f'Agent {agent_idx}  Test Accuracy: {round(test_accuracy * 100, 2)}% ')\n",
        "\n",
        "        return test_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "I2xJyh1kwWCB"
      },
      "outputs": [],
      "source": [
        "# pie_1_values = [1.0]\n",
        "# pie_2_values = np.linspace(0.0, 0.5, 10).tolist()\n",
        "# pie_3_values = [10]\n",
        "\n",
        "# # Use itertools.product to generate all combinations of these values\n",
        "# pie_grid = list(itertools.product(pie_1_values, pie_2_values, pie_3_values))\n",
        "# best_acc = 0.0\n",
        "# best_pie=None\n",
        "\n",
        "# for pi in pie_grid:\n",
        "#   print(f\"Evaluating params {pi}\")\n",
        "#   system = Agents(train_loaders, val_loaders, test_loader, num_agents, iterations=100, pie=pi)\n",
        "#   print(f\"Learning rate that respect diffusion conensus (W=I-εL) is {system.learning_rate}\")\n",
        "#   stop_iteration = system.NEXT_train(tolerance=1e-2)\n",
        "\n",
        "#   print(\"Test accuracy after aggregation process\")\n",
        "#   test_acc = system.test_agent(0)\n",
        "#   if test_acc>best_acc:\n",
        "#     best_acc = test_acc\n",
        "#     best_pie = pi\n",
        "# print(f\"Best pie is {best_pie} with accuracy {best_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "beVywsT5HWU5",
        "outputId": "b461e162-0706-4071-b4e9-8b8b4428e562"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing NEXT training with neighborhood:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Difference equal to -1.3845725059509277\n",
            "Difference equal to -0.05341468006372452\n",
            "Difference equal to -0.04584247246384621\n",
            "Difference equal to -0.039320096373558044\n",
            "Difference equal to -1.0537323951721191\n",
            "Difference equal to 0.21546243131160736\n",
            "Difference equal to 0.05087755620479584\n",
            "Difference equal to -0.036480918526649475\n",
            "Difference equal to -0.025043485686182976\n",
            "Difference equal to -0.09118062257766724\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing NEXT training with neighborhood:  20%|██        | 1/5 [00:15<01:03, 16.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stopping value:0.009118062444031239\n",
            " \n",
            " Average params difference equal to 2.6855196952819824\n",
            "Difference equal to -0.5237858295440674\n",
            "Difference equal to -0.029622459784150124\n",
            "Difference equal to -0.035099096596241\n",
            "Difference equal to -0.03432061895728111\n",
            "Difference equal to -0.38773155212402344\n",
            "Difference equal to 0.18540529906749725\n",
            "Difference equal to 0.03189646080136299\n",
            "Difference equal to -0.026476135477423668\n",
            "Difference equal to -0.0019889231771230698\n",
            "Difference equal to -0.07323754578828812\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing NEXT training with neighborhood:  40%|████      | 2/5 [00:32<00:48, 16.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stopping value:0.007323754485696554\n",
            " \n",
            " Average params difference equal to 1.528389573097229\n",
            "Difference equal to -0.2248922884464264\n",
            "Difference equal to -0.016768623143434525\n",
            "Difference equal to -0.029832366853952408\n",
            "Difference equal to -0.03004767745733261\n",
            "Difference equal to -0.21661360561847687\n",
            "Difference equal to 0.16318055987358093\n",
            "Difference equal to 0.0208304263651371\n",
            "Difference equal to -0.018511546775698662\n",
            "Difference equal to -0.002389230765402317\n",
            "Difference equal to -0.06203284487128258\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing NEXT training with neighborhood:  60%|██████    | 3/5 [00:49<00:33, 16.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stopping value:0.006203284487128258\n",
            " \n",
            " Average params difference equal to 0.5046598315238953\n",
            "Difference equal to -0.09989237785339355\n",
            "Difference equal to -0.010580467991530895\n",
            "Difference equal to -0.02278691530227661\n",
            "Difference equal to -0.02731313742697239\n",
            "Difference equal to 0.024556031450629234\n",
            "Difference equal to 0.14036019146442413\n",
            "Difference equal to 0.009700902737677097\n",
            "Difference equal to -0.011139215901494026\n",
            "Difference equal to -0.012297721579670906\n",
            "Difference equal to -0.05452270060777664\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "system = Agents(train_loaders, val_loaders, test_loader, num_agents, iterations=5, pie=[1.0,0.0,10], tau = 1e4, gamma_zero=0.5)\n",
        "#print(f\"Learning rate that respect diffusion conensus (W=I-εL) is {system.learning_rate}\")\n",
        "system.NEXT_train(tolerance=1e-2)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {round((end_time - start_time) / 60,2)} minutes \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVfUcy9JB1qR",
        "outputId": "14cbd053-04e0-45c4-b470-958c7428f776"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent 0  Test Accuracy: 24.53% \n",
            "Agent 1  Test Accuracy: 16.93% \n",
            "Agent 2  Test Accuracy: 19.67% \n",
            "Agent 3  Test Accuracy: 15.67% \n",
            "Agent 4  Test Accuracy: 18.53% \n",
            "Agent 5  Test Accuracy: 21.4% \n",
            "Agent 6  Test Accuracy: 20.67% \n",
            "Agent 7  Test Accuracy: 16.53% \n",
            "Agent 8  Test Accuracy: 16.8% \n",
            "Agent 9  Test Accuracy: 16.6% \n"
          ]
        }
      ],
      "source": [
        "for agent in range(system.num_agents):\n",
        "    system.test_agent(agent) #se output è runnato"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJzHyKJXTu-S",
        "outputId": "bf8a5e3e-ecc0-453d-d65c-cafb48bcea59"
      },
      "outputs": [],
      "source": [
        "system.models[0].state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doOrJQOlED1f"
      },
      "source": [
        "# Codice senza classi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J898q3MAibET"
      },
      "outputs": [],
      "source": [
        "#NOW NEED TO TEST THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nuK9ZCR71dRk"
      },
      "outputs": [],
      "source": [
        "s = Agents(train_loaders, val_loaders, test_loader, num_agents, epochs=2)\n",
        "\n",
        "# Initializations\n",
        "wk = Net().state_dict()\n",
        "pie_tilde =  [{agent:[]} for agent in range(s.num_agents)]\n",
        "Z = [{agent:[]} for agent in range(s.num_agents)]\n",
        "Y = [{agent:[]} for agent in range(s.num_agents)]\n",
        "\n",
        "for i in range(s.num_agents):\n",
        "    z_k = {k: torch.zeros_like(p) for k, p in wk.items()}\n",
        "    w_previous = s.models[i].state_dict()\n",
        "\n",
        "    for name, param in s.models[i].named_parameters(): #Attention load_state_dict() method doesn't works , manually uopdate here\n",
        "     if name in wk:\n",
        "        param.data = wk[name].clone()\n",
        "    s.models[i].train()\n",
        "    optimizer = s.optimizers[i]\n",
        "    #Compute difference among current iterate and previous response variable (x_i - x[n])\n",
        "    difference = [(w_prev-wk) for (n1,w_prev),(n2,wk) in zip(w_previous.items(), s.models[i].state_dict().items())] #computye and flat difference among w_prev and wk\n",
        "    difference_flatten = torch.cat([diff.view(-1) for diff in difference])\n",
        "    all_batch_gradients = []\n",
        "    print(f\"AGENT {i}\")\n",
        "    print(s.models[i].state_dict())\n",
        "    for inputs, labels in s.train_loaders[i]:\n",
        "        outputs_i = s.models[i](inputs)\n",
        "        loss_i = s.criterion(outputs_i, labels)  # Surrogate loss for agent i solved in closed form\n",
        "\n",
        "        optimizer.zero_grad()  # Clear previous gradients\n",
        "        # Save gradients and flat them in 1D object (we need this to perfom multiplication with difference)\n",
        "        gradients = torch.autograd.grad(loss_i, s.models[i].parameters(), retain_graph=True)\n",
        "        # for name, grad in zip(s.models[i].state_dict().keys(), gradients):\n",
        "        #     y_zero[i][name] += grad.clone().detach()\n",
        "\n",
        "        pie_flatten = torch.cat([grad.view(-1) for grad in gradients ])\n",
        "        all_batch_gradients.append(pie_flatten.clone().detach()) #initialize y_i[k] as ∇f_i[0]\n",
        "        # Add pi_flatten^T * diff_flat to the surrogate loss\n",
        "        loss_surrogate = loss_i.clone().requires_grad_(True)  + torch.matmul(pie_flatten, difference_flatten).requires_grad_(True)\n",
        "        loss_surrogate.backward()  # Compute gradients\n",
        "        optimizer.step()  # Update model parameters\n",
        "    print(\"-----------------\")\n",
        "    print(s.models[i].state_dict())\n",
        "    # Update convex combination z_i[k] variable:\n",
        "    for n, p in z_k.items():\n",
        "        z_k[n] += w_previous[n] + 0.5 * (w_previous[n] - wk[n])  # Update rule with gamma\n",
        "    Z[i]= z_k\n",
        "    Y[i] = torch.mean(torch.stack(all_batch_gradients), dim=0) #gradients mean of batches (not of agents!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRg3zMJW0gDL",
        "outputId": "bc21777a-72fc-41ff-e04d-385fd4d0de0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-0.0156, -0.0333, -0.0230,  ..., -0.0034, -0.0289,  0.0065])"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZhO47CeTso9"
      },
      "outputs": [],
      "source": [
        "new_state_dict = []\n",
        "next = []\n",
        "\n",
        "diff = [{k: torch.zeros_like(p) for k, p in s.models[0].state_dict().items()} for _ in range(s.num_agents)]\n",
        "for i in range(s.num_agents):\n",
        "    #Updates of local variables\n",
        "    # W[k+1]:\n",
        "    updated_state_dict = {k: torch.zeros_like(p) for k, p in s.models[i].state_dict().items()} #updated weights\n",
        "    grads_passing = {k: torch.zeros_like(p) for k, p in s.models[i].state_dict().items()} #consensus among diffusion matrix and y_j[k]\n",
        "\n",
        "    for j in range(s.num_agents):\n",
        "      for (name1, w_i), (name2, z_i) in zip(updated_state_dict.items(), Z[j].items()):\n",
        "          g_bio = g(s.Z[i], s.Z[j], [1,.25,10]).type(z_i.dtype)\n",
        "          updated_state_dict[name1] +=torch.mul( z_i, g_bio) * s.W[i][j]  # Accumulate contributions from each j\n",
        "      #print(f\"Agents: {i,j} with bio coefficient {g_bio}\")\n",
        "    # print(f\"Agent {i}\")\n",
        "    # print(updated_state_dict)\n",
        "\n",
        "    # Load the updated state dictionary back into model i\n",
        "    # for (n1,upd),(n2,prev) in zip(updated_state_dict.items() , s.models[i].state_dict().items()):\n",
        "    #   diff[i][n1] += upd - prev\n",
        "\n",
        "    # new_state_dict.append(updated_state_dict)\n",
        "    # s.models[i].load_state_dict(updated_state_dict)\n",
        "    # for inputs, labels in s.train_loaders[i]:\n",
        "    #     outputs_i = s.models[i](inputs)\n",
        "    #     loss_nextiter = s.criterion(outputs_i, labels)  # Surrogate loss for agent i solved in closed form\n",
        "\n",
        "    #     optimizer.zero_grad()  # Clear previous gradients\n",
        "    #     loss_nextiter.backward(retain_graph=True)  # Compute gradients of iteration [k+1]\n",
        "    #     # Save gradients\n",
        "    #     next_gradients = torch.autograd.grad(loss_nextiter, s.models[i].parameters(), retain_graph=True)\n",
        "    #     next_grad_flat = torch.cat([grad.view(-1) for grad in next_gradients ])\n",
        "    # next.append(torch.mean(torch.stack(all_batch_gradients), dim=0))\n",
        "    # #y[k+1]:\n",
        "    # grads_diff = Y[i] -next_grad_flat[i]\n",
        "    # grads_diff_resh = reshape_flattened_gradients(grads_diff, s.models[i]) #reshape in orginal parameters size\n",
        "    # print(grads_diff_resh)\n",
        "    # for j in range(s.num_agents):\n",
        "    #     if j != i:\n",
        "    #       y_k_ = reshape_flattened_gradients(Y[j], s.models[j])\n",
        "    #       for name, p in y_k_.items():\n",
        "    #         grads_passing[name] += p * s.W[i][j]  # Accumulate contributions from each j\n",
        "    # for (n1, grad),(n2, diff) in zip(grads_passing.items(), grads_diff_resh.items()):\n",
        "    #   grads_passing[n1] = grad + diff\n",
        "    # Y[i] = torch.cat([grad.view(-1) for n,grad in grads_passing.items() ]) #update y_i[k+1]\n",
        "\n",
        "\n",
        "    # # pie_tilde[k]:\n",
        "    # pie_tilde[i] = Y[i] - next[i]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqXdmXJnI9tK",
        "outputId": "a706038c-ac22-48bd-cdcb-cf44e3fe2a7c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([-0.4466, -0.4068, -0.1043,  ..., -0.1398, -0.4319, -0.2688]),\n",
              " tensor([0.6895, 0.6224, 0.5876,  ..., 0.2100, 0.3751, 0.3143]),\n",
              " tensor([-0.2062, -0.2847, -0.2279,  ..., -0.0200,  0.1171, -0.2126]),\n",
              " tensor([2.0243, 2.0186, 1.8945,  ..., 0.5287, 1.3302, 0.8262]),\n",
              " tensor([-0.1963,  0.0836,  0.1435,  ..., -0.1159, -0.0439, -0.0503]),\n",
              " tensor([-0.8225, -0.4581, -0.3458,  ..., -0.6218, -0.3131, -0.3138]),\n",
              " tensor([-0.0614, -0.0961, -0.1151,  ...,  0.0575, -0.0534, -0.1652]),\n",
              " tensor([2.0401, 1.1129, 0.0718,  ..., 0.5995, 0.7635, 0.7039]),\n",
              " tensor([ 1.5397,  0.6460,  0.2590,  ...,  0.0363,  0.1882, -0.0502]),\n",
              " tensor([-0.0020, -0.0067, -0.0045,  ...,  0.2673, -0.0460, -0.0765])]"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pie_tilde"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9aOIoKi7mjCG",
        "lPhJ_wwumW3S",
        "doOrJQOlED1f"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
